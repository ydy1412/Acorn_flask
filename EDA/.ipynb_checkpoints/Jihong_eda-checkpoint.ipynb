{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, cross_val_score, cross_val_predict\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import keras\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/numpy_array_data/Input5_data.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-493300fedcef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mInput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/numpy_array_data/Input5_data.npy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mOutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/numpy_array_data/labels5_data.npy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ydy1412\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    420\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 422\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    423\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/numpy_array_data/Input5_data.npy'"
     ]
    }
   ],
   "source": [
    "Input = np.load('data/numpy_array_data/Input5_data.npy')\n",
    "Output = np.load('data/numpy_array_data/labels5_data.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(Input, Output, test_size=0.2, random_state=0)\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11195, 161)\n",
      "(2799, 161)\n",
      "(11195, 5)\n",
      "(2799, 5)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(output_dim= 3, input_dim = 161, activation= 'relu'))\n",
    "model.add(Dense(output_dim= 3, activation= 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'categorical_crossentropy', optimizer ='sgd', metrics= ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10937/10937 [==============================] - 0s 12us/step - loss: 1.0391 - accuracy: 0.6001\n",
      "Epoch 2/10\n",
      "10937/10937 [==============================] - 0s 10us/step - loss: 0.8088 - accuracy: 0.7782\n",
      "Epoch 3/10\n",
      "10937/10937 [==============================] - 0s 9us/step - loss: 0.7378 - accuracy: 0.7811\n",
      "Epoch 4/10\n",
      "10937/10937 [==============================] - 0s 10us/step - loss: 0.7066 - accuracy: 0.7811\n",
      "Epoch 5/10\n",
      "10937/10937 [==============================] - 0s 10us/step - loss: 0.6919 - accuracy: 0.7811\n",
      "Epoch 6/10\n",
      "10937/10937 [==============================] - 0s 10us/step - loss: 0.6845 - accuracy: 0.7811\n",
      "Epoch 7/10\n",
      "10937/10937 [==============================] - 0s 10us/step - loss: 0.6808 - accuracy: 0.7811\n",
      "Epoch 8/10\n",
      "10937/10937 [==============================] - 0s 10us/step - loss: 0.6788 - accuracy: 0.7811\n",
      "Epoch 9/10\n",
      "10937/10937 [==============================] - 0s 10us/step - loss: 0.6777 - accuracy: 0.7811\n",
      "Epoch 10/10\n",
      "10937/10937 [==============================] - 0s 10us/step - loss: 0.6771 - accuracy: 0.7811\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x203faf94ef0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, nb_epoch= 10, batch_size= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2735/2735 [==============================] - 0s 13us/step\n",
      "loss_and_metrics: [0.6751839505471084, 0.7824497222900391]\n"
     ]
    }
   ],
   "source": [
    "loss_and_metrics = model.evaluate(X_test, y_test, batch_size =100)\n",
    "print('loss_and_metrics: ' +str(loss_and_metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "## OverSampling SMOTE\n",
    "SMOTE(Synthetic Minority Over-sampling Technique) 방법도 ADASYN 방법처럼 데이터를 생성하지만 생성된 데이터를 무조건 소수 클래스라고 하지 않고 분류 모형에 따라 분류한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number transactions X_train dataset:  (10937, 161)\n",
      "Number transactions y_train dataset:  (10937, 3)\n",
      "Number transactions X_test dataset:  (2735, 161)\n",
      "Number transactions y_test dataset:  (2735, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Number transactions X_train dataset: \", X_train.shape) \n",
    "print(\"Number transactions y_train dataset: \", y_train.shape) \n",
    "print(\"Number transactions X_test dataset: \", X_test.shape) \n",
    "print(\"Number transactions y_test dataset: \", y_test.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before OverSampling, counts of label '1': [1261 8543 1133]\n",
      "Before OverSampling, counts of label '0': [9676 2394 9804] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train == 1))) \n",
    "print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train == 0))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After OverSampling, the shape of train_X: (25629, 161)\n",
      "After OverSampling, the shape of train_y: (25629, 3) \n",
      "\n",
      "After OverSampling, counts of label '1': [8543 8543 8543]\n",
      "After OverSampling, counts of label '0': [17086 17086 17086]\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE \n",
    "sm = SMOTE(random_state = 42) \n",
    "X_train_res, y_train_res = sm.fit_sample(X_train, y_train) \n",
    "  \n",
    "print('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape)) \n",
    "print('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape)) \n",
    "  \n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res == 1))) \n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res == 0))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_res, X_test_res, y_train_res, y_test_res = train_test_split(X_train_res, y_train_res , test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정답률:  0.6513850955911041\n",
      "레포트: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.65      0.74      1674\n",
      "           1       0.85      0.62      0.72      1725\n",
      "           2       0.86      0.68      0.76      1727\n",
      "\n",
      "   micro avg       0.86      0.65      0.74      5126\n",
      "   macro avg       0.86      0.65      0.74      5126\n",
      "weighted avg       0.86      0.65      0.74      5126\n",
      " samples avg       0.65      0.65      0.65      5126\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier().fit(X_train_res, y_train_res)\n",
    "pred = clf.predict(X_test_res)\n",
    "\n",
    "print('정답률: ', accuracy_score(y_test_res, pred))\n",
    "print('레포트: \\n', classification_report(y_test_res, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier: 56.40\n"
     ]
    }
   ],
   "source": [
    "knn_model = KNeighborsClassifier()\n",
    "knn_model.fit(X_train_res, y_train_res)\n",
    "\n",
    "y_pred = knn_model.predict(X_test_res)\n",
    "\n",
    "print('KNeighborsClassifier: %.2f'%(metrics.accuracy_score(y_test_res, y_pred)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(output_dim= 3, input_dim = 161, activation= 'relu'))\n",
    "model.add(Dense(output_dim= 3, activation= 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "20503/20503 [==============================] - 0s 13us/step - loss: 0.5993 - accuracy: 0.6911\n",
      "Epoch 2/50\n",
      "20503/20503 [==============================] - 0s 11us/step - loss: 0.5488 - accuracy: 0.7173\n",
      "Epoch 3/50\n",
      "20503/20503 [==============================] - 0s 12us/step - loss: 0.5260 - accuracy: 0.7285\n",
      "Epoch 4/50\n",
      "20503/20503 [==============================] - 0s 12us/step - loss: 0.5132 - accuracy: 0.7337\n",
      "Epoch 5/50\n",
      "20503/20503 [==============================] - 0s 12us/step - loss: 0.5045 - accuracy: 0.7465\n",
      "Epoch 6/50\n",
      "20503/20503 [==============================] - 0s 11us/step - loss: 0.4978 - accuracy: 0.7558\n",
      "Epoch 7/50\n",
      "20503/20503 [==============================] - 0s 12us/step - loss: 0.4927 - accuracy: 0.7610\n",
      "Epoch 8/50\n",
      "20503/20503 [==============================] - 0s 12us/step - loss: 0.4887 - accuracy: 0.7647\n",
      "Epoch 9/50\n",
      "20503/20503 [==============================] - 0s 12us/step - loss: 0.4854 - accuracy: 0.7661\n",
      "Epoch 10/50\n",
      "20503/20503 [==============================] - 0s 12us/step - loss: 0.4828 - accuracy: 0.7680 0s - loss: 0.4809 - accuracy: 0.\n",
      "Epoch 11/50\n",
      "20503/20503 [==============================] - 0s 12us/step - loss: 0.4805 - accuracy: 0.7683\n",
      "Epoch 12/50\n",
      "20503/20503 [==============================] - 0s 11us/step - loss: 0.4786 - accuracy: 0.7699 0s - loss: 0.4773 - accuracy: 0.\n",
      "Epoch 13/50\n",
      "20503/20503 [==============================] - 0s 11us/step - loss: 0.4770 - accuracy: 0.7699\n",
      "Epoch 14/50\n",
      "20503/20503 [==============================] - 0s 12us/step - loss: 0.4756 - accuracy: 0.7707\n",
      "Epoch 15/50\n",
      "20503/20503 [==============================] - 0s 11us/step - loss: 0.4742 - accuracy: 0.7720 0s - loss: 0.4702 - accuracy: \n",
      "Epoch 16/50\n",
      "20503/20503 [==============================] - 0s 12us/step - loss: 0.4732 - accuracy: 0.7719\n",
      "Epoch 17/50\n",
      "20503/20503 [==============================] - 0s 11us/step - loss: 0.4724 - accuracy: 0.7716 0s - loss: 0.4756 - accuracy: 0.\n",
      "Epoch 18/50\n",
      "20503/20503 [==============================] - 0s 12us/step - loss: 0.4717 - accuracy: 0.7724\n",
      "Epoch 19/50\n",
      "20503/20503 [==============================] - 0s 11us/step - loss: 0.4708 - accuracy: 0.7727\n",
      "Epoch 20/50\n",
      "20503/20503 [==============================] - 0s 11us/step - loss: 0.4698 - accuracy: 0.7726\n",
      "Epoch 21/50\n",
      "20503/20503 [==============================] - 0s 12us/step - loss: 0.4695 - accuracy: 0.7725\n",
      "Epoch 22/50\n",
      "20503/20503 [==============================] - 0s 11us/step - loss: 0.4689 - accuracy: 0.7736\n",
      "Epoch 23/50\n",
      "20503/20503 [==============================] - 0s 11us/step - loss: 0.4684 - accuracy: 0.7739\n",
      "Epoch 24/50\n",
      "20503/20503 [==============================] - 0s 12us/step - loss: 0.4676 - accuracy: 0.7733\n",
      "Epoch 25/50\n",
      "20503/20503 [==============================] - 0s 12us/step - loss: 0.4674 - accuracy: 0.7732\n",
      "Epoch 26/50\n",
      "20503/20503 [==============================] - 0s 12us/step - loss: 0.4671 - accuracy: 0.7740\n",
      "Epoch 27/50\n",
      "20503/20503 [==============================] - 0s 12us/step - loss: 0.4665 - accuracy: 0.7744\n",
      "Epoch 28/50\n",
      "20503/20503 [==============================] - 0s 12us/step - loss: 0.4664 - accuracy: 0.7745\n",
      "Epoch 29/50\n",
      "20503/20503 [==============================] - 0s 12us/step - loss: 0.4659 - accuracy: 0.7741\n",
      "Epoch 30/50\n",
      "20503/20503 [==============================] - 0s 12us/step - loss: 0.4654 - accuracy: 0.7745\n",
      "Epoch 31/50\n",
      "20503/20503 [==============================] - 0s 11us/step - loss: 0.4650 - accuracy: 0.7742 0s - loss: 0.4625 - accuracy: 0.\n",
      "Epoch 32/50\n",
      "20503/20503 [==============================] - 0s 12us/step - loss: 0.4648 - accuracy: 0.7743\n",
      "Epoch 33/50\n",
      "20503/20503 [==============================] - 0s 11us/step - loss: 0.4645 - accuracy: 0.7736\n",
      "Epoch 34/50\n",
      "20503/20503 [==============================] - 0s 12us/step - loss: 0.4643 - accuracy: 0.7745\n",
      "Epoch 35/50\n",
      "20503/20503 [==============================] - 0s 11us/step - loss: 0.4642 - accuracy: 0.7756\n",
      "Epoch 36/50\n",
      "20503/20503 [==============================] - 0s 11us/step - loss: 0.4636 - accuracy: 0.7744\n",
      "Epoch 37/50\n",
      "20503/20503 [==============================] - 0s 11us/step - loss: 0.4634 - accuracy: 0.7753\n",
      "Epoch 38/50\n",
      "20503/20503 [==============================] - 0s 11us/step - loss: 0.4632 - accuracy: 0.7757\n",
      "Epoch 39/50\n",
      "20503/20503 [==============================] - 0s 12us/step - loss: 0.4628 - accuracy: 0.7753\n",
      "Epoch 40/50\n",
      "20503/20503 [==============================] - 0s 12us/step - loss: 0.4627 - accuracy: 0.7753\n",
      "Epoch 41/50\n",
      "20503/20503 [==============================] - 0s 11us/step - loss: 0.4622 - accuracy: 0.7766\n",
      "Epoch 42/50\n",
      "20503/20503 [==============================] - 0s 11us/step - loss: 0.4622 - accuracy: 0.7759\n",
      "Epoch 43/50\n",
      "20503/20503 [==============================] - 0s 12us/step - loss: 0.4618 - accuracy: 0.7761\n",
      "Epoch 44/50\n",
      "20503/20503 [==============================] - 0s 13us/step - loss: 0.4616 - accuracy: 0.7761\n",
      "Epoch 45/50\n",
      "20503/20503 [==============================] - 0s 12us/step - loss: 0.4615 - accuracy: 0.7756\n",
      "Epoch 46/50\n",
      "20503/20503 [==============================] - 0s 12us/step - loss: 0.4613 - accuracy: 0.7758\n",
      "Epoch 47/50\n",
      "20503/20503 [==============================] - 0s 11us/step - loss: 0.4612 - accuracy: 0.7765\n",
      "Epoch 48/50\n",
      "20503/20503 [==============================] - 0s 12us/step - loss: 0.4609 - accuracy: 0.7763\n",
      "Epoch 49/50\n",
      "20503/20503 [==============================] - 0s 12us/step - loss: 0.4609 - accuracy: 0.7756\n",
      "Epoch 50/50\n",
      "20503/20503 [==============================] - 0s 12us/step - loss: 0.4608 - accuracy: 0.7767\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x203800b8908>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_res, y_train_res, nb_epoch= 50, batch_size= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5126/5126 [==============================] - 0s 9us/step\n",
      "loss_and_metrics: [0.4785609882393248, 0.7630380392074585]\n"
     ]
    }
   ],
   "source": [
    "loss_and_metrics = model.evaluate(X_test_res, y_test_res, batch_size =100)\n",
    "print('loss_and_metrics: ' +str(loss_and_metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "bad input shape (13672, 161)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-5ced1a2d50fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mxboo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mxboo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mOutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxboo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_res\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'XGBClassifier: %.2f'\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test_res\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[0;32m    699\u001b[0m                 \u001b[0mxgb_options\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"eval_metric\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_le\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mXGBLabelEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m         \u001b[0mtraining_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_le\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    217\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0man\u001b[0m \u001b[0minstance\u001b[0m \u001b[0mof\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m         \"\"\"\n\u001b[1;32m--> 219\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    220\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcolumn_or_1d\u001b[1;34m(y, warn)\u001b[0m\n\u001b[0;32m    758\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    759\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 760\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bad input shape {0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: bad input shape (13672, 161)"
     ]
    }
   ],
   "source": [
    "xboo = XGBClassifier()\n",
    "xboo.fit(Output, Input, eval_metric='error')\n",
    "pred = xboo.predict(X_test_res)\n",
    "\n",
    "print('XGBClassifier: %.2f'%(metrics.accuracy_score(y_test_res, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## OverSampling RandomSampling\n",
    "Random Over Sampling은 소수 클래스의 데이터를 반복해서 넣는 것(replacement)이다. 가중치를 증가시키는 것과 비슷하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before OverSampling, counts of label '1': [1254 8535 1150  199   57]\n",
      "Before OverSampling, counts of label '0': [ 9941  2660 10045 10996 11138] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train == 1))) \n",
    "print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train == 0))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After OverSampling, the shape of train_X: (25629, 161)\n",
      "After OverSampling, the shape of train_y: (25629, 3) \n",
      "\n",
      "After OverSampling, counts of label '1': [8543 8543 8543]\n",
      "After OverSampling, counts of label '0': [17086 17086 17086]\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "X_train_ros, y_train_ros = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "print('After OverSampling, the shape of train_X: {}'.format(X_train_ros.shape)) \n",
    "print('After OverSampling, the shape of train_y: {} \\n'.format(y_train_ros.shape)) \n",
    "  \n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_ros == 1))) \n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_ros == 0))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ros, X_test_ros, y_train_ros, y_test_ros = train_test_split(X_train_ros, y_train_ros , test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정답률:  0.8261802575107297\n",
      "레포트: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.89      0.93      1674\n",
      "           1       0.99      0.68      0.81      1725\n",
      "           2       0.96      0.91      0.93      1727\n",
      "\n",
      "   micro avg       0.97      0.83      0.89      5126\n",
      "   macro avg       0.98      0.83      0.89      5126\n",
      "weighted avg       0.98      0.83      0.89      5126\n",
      " samples avg       0.83      0.83      0.83      5126\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier().fit(X_train_ros, y_train_ros)\n",
    "pred = clf.predict(X_test_ros)\n",
    "\n",
    "print('정답률: ', accuracy_score(y_test_ros, pred))\n",
    "print('레포트: \\n', classification_report(y_test_ros, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "20503/20503 [==============================] - 0s 11us/step - loss: 1.1003 - accuracy: 0.3512\n",
      "Epoch 2/10\n",
      "20503/20503 [==============================] - 0s 9us/step - loss: 1.0951 - accuracy: 0.3708\n",
      "Epoch 3/10\n",
      "20503/20503 [==============================] - 0s 10us/step - loss: 1.0915 - accuracy: 0.3825\n",
      "Epoch 4/10\n",
      "20503/20503 [==============================] - 0s 9us/step - loss: 1.0884 - accuracy: 0.3866\n",
      "Epoch 5/10\n",
      "20503/20503 [==============================] - 0s 10us/step - loss: 1.0854 - accuracy: 0.3927\n",
      "Epoch 6/10\n",
      "20503/20503 [==============================] - 0s 10us/step - loss: 1.0824 - accuracy: 0.3956\n",
      "Epoch 7/10\n",
      "20503/20503 [==============================] - 0s 9us/step - loss: 1.0792 - accuracy: 0.4025\n",
      "Epoch 8/10\n",
      "20503/20503 [==============================] - 0s 10us/step - loss: 1.0759 - accuracy: 0.4043\n",
      "Epoch 9/10\n",
      "20503/20503 [==============================] - 0s 10us/step - loss: 1.0727 - accuracy: 0.4061\n",
      "Epoch 10/10\n",
      "20503/20503 [==============================] - 0s 10us/step - loss: 1.0694 - accuracy: 0.4091\n",
      "5126/5126 [==============================] - 0s 9us/step\n",
      "loss_and_metrics: [1.0690502762747982, 0.4192352592945099]\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "Legu =['l1','l2']\n",
    "hidden_nodes = [10*i for i in range(19)]\n",
    "dropout_value = 0.3\n",
    "dropout_layer = [0,1,2]\n",
    "hidden_layer = [0,1,2,3]\n",
    "initializer = ['']\n",
    "model.add(Dense(output_dim= 3, input_dim = 161, activation= 'relu'))\n",
    "model.add(Dense(output_dim= 3, activation= 'softmax'))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer ='sgd', metrics= ['accuracy'])\n",
    "\n",
    "model.fit(X_train_ros, y_train_ros, epochs= 10, batch_size= 100)\n",
    "\n",
    "loss_and_metrics = model.evaluate(X_test_ros, y_test_ros, batch_size =100)\n",
    "print('loss_and_metrics: ' +str(loss_and_metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Oversampling ADASYN\n",
    "\n",
    "ADASYN(Adaptive Synthetic Sampling) 방법은 소수 클래스 데이터와 그 데이터에서 가장 가까운 k개의 소수 클래스 데이터 중 무작위로 선택된 데이터 사이의 직선상에 가상의 소수 클래스 데이터를 만드는 방법이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After OverSampling, the shape of train_X: (42457, 161)\n",
      "After OverSampling, the shape of train_y: (42457, 5) \n",
      "\n",
      "After OverSampling, counts of label '1': [8561 8535 8353 8488 8520]\n",
      "After OverSampling, counts of label '0': [33896 33922 34104 33969 33937]\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import ADASYN\n",
    "ads = ADASYN(random_state=0)\n",
    "X_train_ads, y_train_ads = ads.fit_resample(X_train, y_train)\n",
    "\n",
    "print('After OverSampling, the shape of train_X: {}'.format(X_train_ads.shape)) \n",
    "print('After OverSampling, the shape of train_y: {} \\n'.format(y_train_ads.shape)) \n",
    "  \n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_ads == 1))) \n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_ads == 0))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ads, X_test_ads, y_train_ads, y_test_ads = train_test_split(X_train_ads, y_train_ads , test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정답률:  0.6122232689590202\n",
      "레포트: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.60      0.71      1692\n",
      "           1       0.85      0.49      0.62      1767\n",
      "           2       0.88      0.62      0.72      1641\n",
      "           3       0.88      0.61      0.72      1697\n",
      "           4       0.82      0.75      0.78      1695\n",
      "\n",
      "   micro avg       0.86      0.61      0.72      8492\n",
      "   macro avg       0.86      0.61      0.71      8492\n",
      "weighted avg       0.86      0.61      0.71      8492\n",
      " samples avg       0.61      0.61      0.61      8492\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier().fit(X_train_ads, y_train_ads)\n",
    "pred = clf.predict(X_test_ads)\n",
    "\n",
    "print('정답률: ', accuracy_score(y_test_ads, pred))\n",
    "print('레포트: \\n', classification_report(y_test_ads, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## SMOTE + ENN\n",
    "SMOTE+ENN 방법은 SMOTE(Synthetic Minority Over-sampling Technique) 방법과 ENN(Edited Nearest Neighbours) 방법을 섞은 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After OverSampling, the shape of train_X: (17602, 161)\n",
      "After OverSampling, the shape of train_y: (17602, 5) \n",
      "\n",
      "After OverSampling, counts of label '1': [5760 1876 5059 2919 1988]\n",
      "After OverSampling, counts of label '0': [11842 15726 12543 14683 15614]\n"
     ]
    }
   ],
   "source": [
    "from imblearn.combine import SMOTEENN\n",
    "senn = SMOTEENN(random_state=0)\n",
    "X_train_senn, y_train_senn = senn.fit_resample(X_train, y_train)\n",
    "\n",
    "print('After OverSampling, the shape of train_X: {}'.format(X_train_senn.shape)) \n",
    "print('After OverSampling, the shape of train_y: {} \\n'.format(y_train_senn.shape)) \n",
    "  \n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_senn == 1))) \n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_senn == 0))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_senn, X_test_senn, y_train_senn, y_test_senn = train_test_split(X_train_senn, y_train_senn , test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정답률:  0.8702073274637887\n",
      "레포트: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.90      0.93      1160\n",
      "           1       0.97      0.49      0.66       366\n",
      "           2       0.96      0.88      0.92      1049\n",
      "           3       1.00      0.97      0.98       583\n",
      "           4       1.00      0.99      0.99       363\n",
      "\n",
      "   micro avg       0.97      0.87      0.92      3521\n",
      "   macro avg       0.98      0.85      0.90      3521\n",
      "weighted avg       0.97      0.87      0.91      3521\n",
      " samples avg       0.87      0.87      0.87      3521\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier().fit(X_train_senn, y_train_senn)\n",
    "pred = clf.predict(X_test_senn)\n",
    "\n",
    "print('정답률: ', accuracy_score(y_test_senn, pred))\n",
    "print('레포트: \\n', classification_report(y_test_senn, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_87 to have shape (1,) but got array with shape (5,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-c089dd654611>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m'sgd'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_senn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_senn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mloss_and_metrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_senn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_senn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1154\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1156\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    619\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 621\u001b[1;33m                 exception_prefix='target')\n\u001b[0m\u001b[0;32m    622\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    623\u001b[0m             \u001b[1;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    143\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    146\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected dense_87 to have shape (1,) but got array with shape (5,)"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(output_dim= 5, input_dim = 161, activation= 'relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(output_dim= 5, activation= 'relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(output_dim= 5, activation= 'softmax'))\n",
    "\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer ='sgd', metrics= ['accuracy'])\n",
    "\n",
    "model.fit(X_train_senn, y_train_senn, epochs= 30, batch_size= 100)\n",
    "\n",
    "loss_and_metrics = model.evaluate(X_test_senn, y_test_senn, batch_size =100)\n",
    "print('loss_and_metrics: ' +str(loss_and_metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_75 to have shape (1,) but got array with shape (5,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-70872b125052>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_senn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_senn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mloss_and_metrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_senn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_senn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1154\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1156\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    619\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 621\u001b[1;33m                 exception_prefix='target')\n\u001b[0m\u001b[0;32m    622\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    623\u001b[0m             \u001b[1;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    143\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    146\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected dense_75 to have shape (1,) but got array with shape (5,)"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(output_dim= 5, input_dim = 161, activation= 'relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(output_dim= 5, activation= 'relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(output_dim= 5, activation= 'softmax'))\n",
    "\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer ='adam', metrics= ['accuracy'])\n",
    "\n",
    "model.fit(X_train_senn, y_train_senn, epochs= 300, batch_size= 100)\n",
    "\n",
    "loss_and_metrics = model.evaluate(X_test_senn, y_test_senn, batch_size =100)\n",
    "print('loss_and_metrics: ' +str(loss_and_metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## SMOTE + Tomek\n",
    "SMOTE+Tomek 방법은 SMOTE(Synthetic Minority Over-sampling Technique) 방법과 토멕링크 방법을 섞은 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "senn = SMOTETomek(random_state=0)\n",
    "X_train_stek, y_train_stek = senn.fit_resample(X_train, y_train)\n",
    "\n",
    "print('After OverSampling, the shape of train_X: {}'.format(X_train_stek.shape)) \n",
    "print('After OverSampling, the shape of train_y: {} \\n'.format(y_train_stek.shape)) \n",
    "  \n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_stek == 1))) \n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_stek == 0))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_stek, X_test_stek, y_train_stek, y_test_stek = train_test_split(X_train_stek, y_train_stek , test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정답률:  0.6427032321253673\n",
      "레포트: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.65      0.74      1645\n",
      "           1       0.85      0.58      0.69      1773\n",
      "           2       0.86      0.70      0.77      1687\n",
      "\n",
      "   micro avg       0.86      0.64      0.74      5105\n",
      "   macro avg       0.86      0.64      0.74      5105\n",
      "weighted avg       0.86      0.64      0.73      5105\n",
      " samples avg       0.64      0.64      0.64      5105\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier().fit(X_train_stek, y_train_stek)\n",
    "pred = clf.predict(X_test_stek)\n",
    "\n",
    "print('정답률: ', accuracy_score(y_test_stek, pred))\n",
    "print('레포트: \\n', classification_report(y_test_stek, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "## UnderSampling NearMiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Undersampling, counts of label '1': [1261 8543 1133]\n",
      "Before Undersampling, counts of label '0': [9676 2394 9804] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Before Undersampling, counts of label '1': {}\".format(sum(y_train == 1))) \n",
    "print(\"Before Undersampling, counts of label '0': {} \\n\".format(sum(y_train == 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Undersampling, the shape of train_X: (3399, 161)\n",
      "After Undersampling, the shape of train_y: (3399, 3) \n",
      "\n",
      "After Undersampling, counts of label '1': [1133 1133 1133]\n",
      "After Undersampling, counts of label '0': [2266 2266 2266]\n"
     ]
    }
   ],
   "source": [
    "# apply near miss \n",
    "from imblearn.under_sampling import NearMiss \n",
    "nr = NearMiss() \n",
    "  \n",
    "X_train_miss, y_train_miss = nr.fit_sample(X_train, y_train) \n",
    "  \n",
    "print('After Undersampling, the shape of train_X: {}'.format(X_train_miss.shape)) \n",
    "print('After Undersampling, the shape of train_y: {} \\n'.format(y_train_miss.shape)) \n",
    "  \n",
    "print(\"After Undersampling, counts of label '1': {}\".format(sum(y_train_miss == 1))) \n",
    "print(\"After Undersampling, counts of label '0': {}\".format(sum(y_train_miss == 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정답률:  0.19524680073126144\n",
      "레포트: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.13      0.34      0.18       295\n",
      "           1       0.79      0.15      0.25      2140\n",
      "           2       0.13      0.39      0.19       300\n",
      "\n",
      "   micro avg       0.25      0.20      0.22      2735\n",
      "   macro avg       0.35      0.29      0.21      2735\n",
      "weighted avg       0.65      0.20      0.24      2735\n",
      " samples avg       0.20      0.20      0.20      2735\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier().fit(X_train_miss, y_train_miss)\n",
    "pred = clf.predict(X_test)\n",
    "\n",
    "print('정답률: ', accuracy_score(y_test, pred))\n",
    "print('레포트: \\n', classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
